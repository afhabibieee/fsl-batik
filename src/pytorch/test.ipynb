{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "curr_path = os.path.dirname(os.path.abspath(__name__))\n",
    "if curr_path not in sys.path:\n",
    "    sys.path.insert(0, curr_path)\n",
    "\n",
    "from configs import IMAGE_SIZE, DEVICE, BEST_MODEL_DIR\n",
    "from data.fewshotdataloader import generate_loader\n",
    "from models.protonet import PrototypicalNetwork\n",
    "\n",
    "from torchmetrics.classification import MulticlassRecall, MulticlassPrecision\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined validation and test class\n",
    "\n",
    "with open('../../src/pytorch/data/val.json') as f:\n",
    "    json_val = json.load(f)\n",
    "with open('../../src/pytorch/data/test.json') as f:\n",
    "    json_test = json.load(f)\n",
    "\n",
    "json_test['class_names'] += json_val['class_names']\n",
    "json_test['class_roots'] += json_val['class_roots']\n",
    "\n",
    "with open('../../src/pytorch/data/test2.json', 'w') as f:\n",
    "    json.dump(json_test, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_per_task(\n",
    "    model,\n",
    "    support_images, support_labels,\n",
    "    query_images, query_labels\n",
    "):\n",
    "    classification_scores = model(support_images, support_labels, query_images)\n",
    "    correct = (torch.max(classification_scores.detach().data, 1)[1] == query_labels).sum().item()\n",
    "    total = query_labels.shape[0]\n",
    "\n",
    "    return classification_scores, correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, n_way):\n",
    "    total_pred = 0\n",
    "    correct_pred = 0\n",
    "    total_recall = 0\n",
    "    total_precision = 0\n",
    "\n",
    "    recall = MulticlassRecall(num_classes=n_way, average='macro')\n",
    "    precision = MulticlassPrecision(num_classes=n_way, average='macro')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for support_images, support_labels, query_images, query_labels, _ in data_loader:\n",
    "            classification_scores, correct, total = evaluate_per_task(\n",
    "                model,\n",
    "                support_images.to(DEVICE), support_labels.to(DEVICE),\n",
    "                query_images.to(DEVICE), query_labels.to(DEVICE)\n",
    "            )\n",
    "            correct_pred += correct\n",
    "            total_pred += total\n",
    "\n",
    "            top_scores, pred_labels = torch.max(classification_scores.data, 1)\n",
    "\n",
    "            total_recall += recall(pred_labels, query_labels).item()\n",
    "            total_precision += precision(pred_labels, query_labels).item()\n",
    "    \n",
    "    avg_accuracy = correct_pred/total_pred\n",
    "    avg_recall = total_recall/len(data_loader)\n",
    "    avg_precision = total_precision/len(data_loader)\n",
    "\n",
    "    return avg_accuracy, avg_recall, avg_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(model, n_ways, n_shots, metrics):\n",
    "    results = []\n",
    "    for n_way in n_ways:\n",
    "        result = []\n",
    "        for n_shot in n_shots:\n",
    "            test_loader = generate_loader(\n",
    "                'test2',\n",
    "                image_size=IMAGE_SIZE,\n",
    "                n_way=n_way,\n",
    "                n_shot=n_shot,\n",
    "                n_query=10,\n",
    "                n_task=200,\n",
    "                n_workers=2\n",
    "            )\n",
    "            avg_accuracy, avg_recall, avg_precision = evaluate(model, test_loader, n_way)\n",
    "            result.extend([avg_accuracy, avg_recall, avg_precision])\n",
    "        results.append(result)\n",
    "    \n",
    "    header = [\n",
    "        [f'{shot}-shot' for shot in n_shots for i in range(len(metrics))],\n",
    "        [*[metric for metric in metrics]*len(n_shots)]\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        results,\n",
    "        index=[f'{way}-way' for way in n_ways],\n",
    "        columns = header\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download artifact model and load model\n",
    "def get_embedding(run_id, backend):\n",
    "    artifact_path = os.path.join(BEST_MODEL_DIR, run_id)\n",
    "    if not os.path.exists(artifact_path):\n",
    "        mlflow.artifacts.download_artifacts(\n",
    "            run_id=run_id,\n",
    "            artifact_path='model',\n",
    "            dst_path=artifact_path\n",
    "        )\n",
    "    embedding = PrototypicalNetwork(\n",
    "        'convnext_tiny',\n",
    "        mode='eval'\n",
    "    ).to(DEVICE)\n",
    "    embedding = torch.compile(embedding, backend=backend)\n",
    "    embedding.load_state_dict(torch.load(os.path.join(artifact_path, 'model/model.pt'), map_location=DEVICE))\n",
    "    embedding = embedding.eval()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ways = [3, 6]\n",
    "n_shots = [3, 10, 20]\n",
    "metrics = ['accuracy', 'precision', 'recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = '9f3784aa59224e52bbf2fef98656a9e6'\n",
    "backend = ['eager', 'inductor']\n",
    "model = get_embedding(run_id, backend[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = benchmark(model, n_ways, n_shots, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">3-shot</th>\n",
       "      <th colspan=\"3\" halign=\"left\">10-shot</th>\n",
       "      <th colspan=\"3\" halign=\"left\">20-shot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3-way</th>\n",
       "      <td>0.761833</td>\n",
       "      <td>0.761833</td>\n",
       "      <td>0.783158</td>\n",
       "      <td>0.7965</td>\n",
       "      <td>0.7965</td>\n",
       "      <td>0.809349</td>\n",
       "      <td>0.831500</td>\n",
       "      <td>0.831500</td>\n",
       "      <td>0.840766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6-way</th>\n",
       "      <td>0.590583</td>\n",
       "      <td>0.590583</td>\n",
       "      <td>0.613054</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>0.694714</td>\n",
       "      <td>0.696917</td>\n",
       "      <td>0.696917</td>\n",
       "      <td>0.708406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         3-shot                      10-shot                       20-shot   \n",
       "       accuracy precision    recall accuracy precision    recall  accuracy   \n",
       "3-way  0.761833  0.761833  0.783158   0.7965    0.7965  0.809349  0.831500  \\\n",
       "6-way  0.590583  0.590583  0.613054   0.6795    0.6795  0.694714  0.696917   \n",
       "\n",
       "                           \n",
       "      precision    recall  \n",
       "3-way  0.831500  0.840766  \n",
       "6-way  0.696917  0.708406  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "157c40127fe169e261276f67e74fd5229e97b77c6b3a955a3c02155803032c67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
